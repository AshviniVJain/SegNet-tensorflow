{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import math\n",
    "import skimage\n",
    "import skimage.io\n",
    "\n",
    "IMAGE_HEIGHT = 360\n",
    "IMAGE_WIDTH = 480\n",
    "IMAGE_DEPTH = 3\n",
    "\n",
    "NUM_CLASSES = 11\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 367\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TEST = 101\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 1\n",
    "\n",
    "def _generate_image_and_label_batch(image, label, min_queue_examples,\n",
    "                                    batch_size, shuffle):\n",
    "  \"\"\"Construct a queued batch of images and labels.\n",
    "\n",
    "  Args:\n",
    "    image: 3-D Tensor of [height, width, 3] of type.float32.\n",
    "    label: 3-D Tensor of [height, width, 1] type.int32\n",
    "    min_queue_examples: int32, minimum number of samples to retain\n",
    "      in the queue that provides of batches of examples.\n",
    "    batch_size: Number of images per batch.\n",
    "    shuffle: boolean indicating whether to use a shuffling queue.\n",
    "\n",
    "  Returns:\n",
    "    images: Images. 4D tensor of [batch_size, height, width, 3] size.\n",
    "    labels: Labels. 3D tensor of [batch_size, height, width ,1] size.\n",
    "  \"\"\"\n",
    "  # Create a queue that shuffles the examples, and then\n",
    "  # read 'batch_size' images + labels from the example queue.\n",
    "  num_preprocess_threads = 1\n",
    "  if shuffle:\n",
    "    images, label_batch = tf.train.shuffle_batch(\n",
    "        [image, label],\n",
    "        batch_size=batch_size,\n",
    "        num_threads=num_preprocess_threads,\n",
    "        capacity=min_queue_examples + 3 * batch_size,\n",
    "        min_after_dequeue=min_queue_examples)\n",
    "  else:\n",
    "    images, label_batch = tf.train.batch(\n",
    "        [image, label],\n",
    "        batch_size=batch_size,\n",
    "        num_threads=num_preprocess_threads,\n",
    "        capacity=min_queue_examples + 3 * batch_size)\n",
    "\n",
    "  # Display the training images in the visualizer.\n",
    "  # tf.image_summary('images', images)\n",
    "\n",
    "  return images, label_batch\n",
    "\n",
    "def CamVid_reader_seq(filename_queue, seq_length):\n",
    "  image_seq_filenames = tf.split(axis=0, num_or_size_splits=seq_length, value=filename_queue[0])\n",
    "  label_seq_filenames = tf.split(axis=0, num_or_size_splits=seq_length, value=filename_queue[1])\n",
    "\n",
    "  image_seq = []\n",
    "  label_seq = []\n",
    "  for im ,la in zip(image_seq_filenames, label_seq_filenames):\n",
    "    imageValue = tf.read_file(tf.squeeze(im))\n",
    "    labelValue = tf.read_file(tf.squeeze(la))\n",
    "    image_bytes = tf.image.decode_png(imageValue)\n",
    "    label_bytes = tf.image.decode_png(labelValue)\n",
    "    image = tf.cast(tf.reshape(image_bytes, (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH)), tf.float32)\n",
    "    label = tf.cast(tf.reshape(label_bytes, (IMAGE_HEIGHT, IMAGE_WIDTH, 1)), tf.int64)\n",
    "    image_seq.append(image)\n",
    "    label_seq.append(label)\n",
    "  return image_seq, label_seq\n",
    "\n",
    "def CamVid_reader(filename_queue):\n",
    "\n",
    "  image_filename = filename_queue[0]\n",
    "  label_filename = filename_queue[1]\n",
    "\n",
    "  imageValue = tf.read_file(image_filename)\n",
    "  labelValue = tf.read_file(label_filename)\n",
    "\n",
    "  image_bytes = tf.image.decode_png(imageValue)\n",
    "  label_bytes = tf.image.decode_png(labelValue)\n",
    "\n",
    "  image = tf.reshape(image_bytes, (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH))\n",
    "  label = tf.reshape(label_bytes, (IMAGE_HEIGHT, IMAGE_WIDTH, 1))\n",
    "\n",
    "  return image, label\n",
    "#This is the first part that we need to run to get the names\n",
    "def get_filename_list(path):\n",
    "  fd = open(path)\n",
    "  image_filenames = []\n",
    "  label_filenames = []\n",
    "  filenames = []\n",
    "  for i in fd:\n",
    "    i = i.strip().split(\" \")\n",
    "    image_filenames.append(i[0])\n",
    "    label_filenames.append(i[1])\n",
    "  return image_filenames, label_filenames\n",
    "\n",
    "#This is the second part that we need to run to get filename_queue which is a tensor\n",
    "def CamVidInputs(image_filenames, label_filenames, batch_size):\n",
    "\n",
    "  images = ops.convert_to_tensor(image_filenames, dtype=dtypes.string)\n",
    "  labels = ops.convert_to_tensor(label_filenames, dtype=dtypes.string)\n",
    "\n",
    "  filename_queue = tf.train.slice_input_producer([images, labels], shuffle=True)\n",
    "  \n",
    "    #Then we need CamVid_reader to get the image and label.\n",
    "  image, label = CamVid_reader(filename_queue)\n",
    "  reshaped_image = tf.cast(image, tf.float32)\n",
    "\n",
    "  min_fraction_of_examples_in_queue = 0.4\n",
    "  min_queue_examples = int(NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN *\n",
    "                           min_fraction_of_examples_in_queue)\n",
    "  print ('Filling queue with %d CamVid images before starting to train. '\n",
    "         'This will take a few minutes.' % min_queue_examples)\n",
    "\n",
    "  # Generate a batch of images and labels by building up a queue of examples.\n",
    "  return _generate_image_and_label_batch(reshaped_image, label,\n",
    "                                         min_queue_examples, batch_size,\n",
    "                                         shuffle=True)\n",
    "def get_all_test_data(im_list, la_list):\n",
    "  images = []\n",
    "  labels = []\n",
    "  index = 0\n",
    "  for im_filename, la_filename in zip(im_list, la_list):\n",
    "    im = np.array(skimage.io.imread(im_filename), np.float32)\n",
    "    im = im[np.newaxis]\n",
    "    la = skimage.io.imread(la_filename)\n",
    "    la = la[np.newaxis]\n",
    "    la = la[...,np.newaxis]\n",
    "    images.append(im)\n",
    "    labels.append(la)\n",
    "  return images, labels\n",
    "path = '../SegNet/CamVid/train.txt'\n",
    "batch_size = 5\n",
    "# image_filenames,label_filenames=get_filename_list(path)\n",
    "# image,label_batch = CamVidInputs(image_filenames,label_filenames,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load segnet_model.py\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import math\n",
    "\n",
    "VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "shape_specific=[360,480,1]\n",
    "\n",
    "\n",
    "def vgg_param_load(vgg16_npy_path): \n",
    "    vgg_param_dict = np.load(vgg16_npy_path,encoding='latin1').item()\n",
    "    return vgg_param_dict\n",
    "    for key in vgg_param_dict:\n",
    "        print(key,vgg_param_dict[key][0].shape,vgg_param_dict[key][1].shape)\n",
    "    print(\"vgg parameter loaded\")\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg16_npy_path = \"vgg16.npy\"\n",
    "vgg_param_dict = vgg_param_load(vgg16_npy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_pool(bottom, name):\n",
    "    return tf.nn.avg_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "\n",
    "def max_pool(inputs,name):\n",
    "    value,index = tf.nn.max_pool_with_argmax(inputs,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME',name=name)\n",
    "    print('value shape',value.shape)\n",
    "    print('index shape',index.shape)                          \n",
    "    return value,index,inputs.get_shape().as_list()\n",
    "#here value is the max value, index is the corresponding index, the detail information is here https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/nn/max_pool_with_argmax\n",
    "    \n",
    "def conv_layer(bottom, name, training_state):\n",
    "    with tf.variable_scope(name):\n",
    "        filt = get_conv_filter(name)\n",
    "        conv = tf.nn.conv2d(bottom, filt, [1, 1, 1, 1], padding='SAME')\n",
    "        conv_biases = get_bias(name)\n",
    "        bias = tf.nn.bias_add(conv, conv_biases)\n",
    "        out = batch_norm(bias,training_state,name)\n",
    "            \n",
    "        relu = tf.nn.relu(out)\n",
    "        print(relu)\n",
    "        return relu\n",
    "        #norm is used to identify if we should use batch normalization!\n",
    "        \n",
    "def batch_norm(bias_input, is_training, scope):\n",
    "    if is_training is True:\n",
    "        return tf.contrib.layers.batch_norm(bias_input,is_training = True,center = False,\n",
    "                                                      scope = scope+\"_bn\")\n",
    "    else:\n",
    "        return tf.contrib.layers.batch_norm(bias_input,is_training = False,center = False,\n",
    "                                                      scope = scope+\"_bn\", reuse = True)\n",
    "#is_training = Ture, it will accumulate the statistics of the movements into moving_mean and moving_variance. When it's \n",
    "#not in a training mode, then it would use the values of the moving_mean, and moving_variance. Which is exactly what we want,\n",
    "#since when it's not training, we are not allowed to use the actual mean and variance for the validation data. \n",
    "#reuse is that if we will reuse the layers, which I really don't understand what does that mean\n",
    "\n",
    "def get_conv_filter(name):\n",
    "    return tf.constant(vgg_param_dict[name][0], name=\"filter\")\n",
    "    #so here load the weight for VGG-16, which is kernel, the kernel size for different covolution layers will show in function\n",
    "    #vgg_param_load\n",
    "\n",
    "def get_bias(name):\n",
    "    return tf.constant(vgg_param_dict[name][1], name=\"biases\")\n",
    "    #here load the bias for VGG-16, the bias size will be 64,128,256,512,512, also shown in function vgg_param_load\n",
    "    \n",
    "\n",
    "def unravel_index(indices, shape):\n",
    "    with tf.name_scope('unravel_index'):\n",
    "        indices = tf.to_int64(tf.expand_dims(indices, 0))\n",
    "        shape = tf.to_int64(tf.expand_dims(shape, 1))\n",
    "        strides = tf.to_int64(tf.cumprod(shape, reverse=True))\n",
    "        strides_shifted = tf.to_int64(tf.cumprod(shape, exclusive=True, reverse=True))\n",
    "        return (indices % strides) // strides_shifted\n",
    "    #This function is utilized to transform the flattened maxpooling index to the original 4D tensor, and have already test\n",
    "    #it, which works brilliant!\n",
    "    \"\"\"\n",
    "    indices: indices will be the output index from maxpooling, just to make sure if it's only 1D!\n",
    "    Shape: the shape of the original data,[batch_size,height,width,Num_of_Channels]\n",
    "    output: It's the 4D maxpooling indices!\n",
    "    \"\"\"\n",
    "    \n",
    "def up_sampling(max_values,max_indices,shape):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    value: the maximum value from maxpooling function, value need to be a tensor. The most important thing for\n",
    "    value is that it needs to be reshaped to be only one column! \n",
    "    indices: the flattened position for the maximum value from maxpooling function. Also indices need to be reshaped\n",
    "    to be two dimension. [Num_tot_values,4]. 4 is because we have 4 dimension\n",
    "    shape: the shape of the original data, [batch_size,height,width,Num_of_Channels]\n",
    "    Outputs:\n",
    "    up_sample_sp: The sparse matrix from the up_sampling.\n",
    "    \"\"\"\n",
    "    values_reshape = tf.reshape(max_values,[-1])\n",
    "    indices_reshape = tf.reshape(max_indices,[-1])\n",
    "    print('The shape of reshaped maxindex',indices_reshape.shape)\n",
    "    pooling_index_4d = tf.stack(tf.unstack(unravel_index(indices_reshape,shape), axis=0), axis=1)\n",
    "    print('The shape of 4d indices', pooling_index_4d.shape)\n",
    "    sp_tensor = tf.SparseTensor(pooling_index_4d, values = values_reshape, dense_shape = shape)\n",
    "    sp_dense = tf.sparse_tensor_to_dense(sp_tensor, validate_indices=False)\n",
    "    print('The shape of sparse matrix', sp_dense.shape)\n",
    "    return sp_dense \n",
    "\n",
    "\n",
    "def _initialization(k,c):\n",
    "    \"\"\"\n",
    "    Here the reference paper is https:arxiv.org/pdf/1502.01852\n",
    "    k is the filter size\n",
    "    c is the number of input channels in the filter tensor\n",
    "    we assume for all the layers, the Kernel Matrix follows a gaussian distribution N(0, \\sqrt(2/nl)), where nl is \n",
    "    the total number of units in the input, k^2c, k is the spartial filter size and c is the number of input channels. \n",
    "    Output:\n",
    "    The initialized weight\n",
    "    \"\"\"\n",
    "    std = math.sqrt(2. / (k**2 * c))\n",
    "    return tf.truncated_normal_initializer(stddev = std)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def deconv_layer(inputs,kernel_size,output_shape,name,training_state):\n",
    "    \"\"\"\n",
    "    This deconv_layer is utilized to convolve with the upsampled output, and also layer output\n",
    "    output_shape is different for different layers\n",
    "    The kernel_size = [height,width,output_channel,input_channel]\n",
    "    \"\"\"\n",
    "    bias_shape = output_shape[-1]\n",
    "    weight_shape = kernel_size\n",
    "    k = kernel_size[0]\n",
    "    c = kernel_size[3]\n",
    "    with tf.variable_scope(name):\n",
    "        weights = tf.get_variable(name+\"weight\",shape=weight_shape,initializer = _initialization(k,c))\n",
    "        bias = tf.get_variable(name+\"bias\",shape = bias_shape, initializer = tf.constant_initializer(0.1))\n",
    "        deconv = tf.nn.conv2d_transpose(inputs,weights,output_shape = output_shape,strides = [1,1,1,1],padding='SAME')\n",
    "        bias = tf.nn.bias_add(deconv, bias)\n",
    "        out = batch_norm(bias,training_state,name)\n",
    "         \n",
    "        relu = tf.nn.relu(out)\n",
    "        print(relu)\n",
    "        return relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_files, lab_files = get_filename_list('../SegNet/CamVid/test.txt')\n",
    "im_files = [\"..\" + n for n in im_files]\n",
    "lab_files = [\"..\" + n for n in lab_files]\n",
    "\n",
    "\n",
    "image, labels = get_all_test_data(im_files, lab_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv1_1/Relu:0\", shape=(1, 360, 480, 64), dtype=float32)\n",
      "Tensor(\"conv1_2/Relu:0\", shape=(1, 360, 480, 64), dtype=float32)\n",
      "value shape (1, 180, 240, 64)\n",
      "index shape (1, 180, 240, 64)\n",
      "Tensor(\"conv2_1/Relu:0\", shape=(1, 180, 240, 128), dtype=float32)\n",
      "Tensor(\"conv2_2/Relu:0\", shape=(1, 180, 240, 128), dtype=float32)\n",
      "value shape (1, 90, 120, 128)\n",
      "index shape (1, 90, 120, 128)\n",
      "Tensor(\"conv3_1/Relu:0\", shape=(1, 90, 120, 256), dtype=float32)\n",
      "Tensor(\"conv3_2/Relu:0\", shape=(1, 90, 120, 256), dtype=float32)\n",
      "Tensor(\"conv3_3/Relu:0\", shape=(1, 90, 120, 256), dtype=float32)\n",
      "value shape (1, 45, 60, 256)\n",
      "index shape (1, 45, 60, 256)\n",
      "Tensor(\"conv4_1/Relu:0\", shape=(1, 45, 60, 512), dtype=float32)\n",
      "Tensor(\"conv4_2/Relu:0\", shape=(1, 45, 60, 512), dtype=float32)\n",
      "Tensor(\"conv4_3/Relu:0\", shape=(1, 45, 60, 512), dtype=float32)\n",
      "value shape (1, 23, 30, 512)\n",
      "index shape (1, 23, 30, 512)\n",
      "Tensor(\"conv5_1/Relu:0\", shape=(1, 23, 30, 512), dtype=float32)\n",
      "Tensor(\"conv5_2/Relu:0\", shape=(1, 23, 30, 512), dtype=float32)\n",
      "Tensor(\"conv5_3/Relu:0\", shape=(1, 23, 30, 512), dtype=float32)\n",
      "value shape (1, 12, 15, 512)\n",
      "index shape (1, 12, 15, 512)\n",
      "The shape of reshaped maxindex (92160,)\n",
      "The shape of 4d indices (92160, 4)\n",
      "The shape of sparse matrix (1, 23, 30, 512)\n",
      "Tensor(\"deconv1_2/Relu:0\", shape=(1, 23, 30, 512), dtype=float32)\n",
      "Tensor(\"deconv1_3/Relu:0\", shape=(1, 23, 30, 512), dtype=float32)\n",
      "Tensor(\"deconv1_4/Relu:0\", shape=(1, 23, 30, 512), dtype=float32)\n",
      "The shape of reshaped maxindex (353280,)\n",
      "The shape of 4d indices (353280, 4)\n",
      "The shape of sparse matrix (1, 45, 60, 512)\n",
      "Tensor(\"deconv2_2/Relu:0\", shape=(1, 45, 60, 512), dtype=float32)\n",
      "Tensor(\"deconv2_3/Relu:0\", shape=(1, 45, 60, 512), dtype=float32)\n",
      "Tensor(\"deconv2_4/Relu:0\", shape=(1, 45, 60, 256), dtype=float32)\n",
      "The shape of reshaped maxindex (691200,)\n",
      "The shape of 4d indices (691200, 4)\n",
      "The shape of sparse matrix (1, 90, 120, 256)\n",
      "Tensor(\"deconv3_2/Relu:0\", shape=(1, 90, 120, 256), dtype=float32)\n",
      "Tensor(\"deconv3_3/Relu:0\", shape=(1, 90, 120, 256), dtype=float32)\n",
      "Tensor(\"deconv3_4/Relu:0\", shape=(1, 90, 120, 128), dtype=float32)\n",
      "The shape of reshaped maxindex (1382400,)\n",
      "The shape of 4d indices (1382400, 4)\n",
      "The shape of sparse matrix (1, 180, 240, 128)\n",
      "Tensor(\"deconv4_2/Relu:0\", shape=(1, 180, 240, 128), dtype=float32)\n",
      "Tensor(\"deconv4_3/Relu:0\", shape=(1, 180, 240, 64), dtype=float32)\n",
      "The shape of reshaped maxindex (2764800,)\n",
      "The shape of 4d indices (2764800, 4)\n",
      "The shape of sparse matrix (1, 360, 480, 64)\n",
      "Tensor(\"deconv5_2/Relu:0\", shape=(1, 360, 480, 64), dtype=float32)\n",
      "Tensor(\"deconv5_3/Relu:0\", shape=(1, 360, 480, 11), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "images = image[0]\n",
    "training_state = True\n",
    "conv1_1 = conv_layer(images, \"conv1_1\",training_state)\n",
    "conv1_2 = conv_layer(conv1_1, \"conv1_2\",training_state)\n",
    "pool1,pool1_index,shape_1 = max_pool(conv1_2, 'pool1')\n",
    "    \n",
    "    #Second box of covolution layer(4)\n",
    "conv2_1 = conv_layer(pool1, \"conv2_1\",training_state)\n",
    "conv2_2 = conv_layer(conv2_1, \"conv2_2\",training_state)\n",
    "pool2,pool2_index,shape_2 = max_pool(conv2_2, 'pool2')\n",
    "    \n",
    "    #Third box of covolution layer(7)\n",
    "conv3_1 = conv_layer(pool2, \"conv3_1\",training_state)\n",
    "conv3_2 = conv_layer(conv3_1, \"conv3_2\",training_state)\n",
    "conv3_3 = conv_layer(conv3_2, \"conv3_3\",training_state)\n",
    "pool3,pool3_index,shape_3 = max_pool(conv3_3, 'pool3')\n",
    "    \n",
    "    #Fourth box of covolution layer(10)\n",
    "conv4_1 = conv_layer(pool3, \"conv4_1\",training_state)\n",
    "conv4_2 = conv_layer(conv4_1, \"conv4_2\",training_state)\n",
    "conv4_3 = conv_layer(conv4_2, \"conv4_3\",training_state)\n",
    "pool4,pool4_index,shape_4 = max_pool(conv4_3, 'pool4')\n",
    "\n",
    "    #Fifth box of covolution layers(13)\n",
    "conv5_1 = conv_layer(pool4, \"conv5_1\",training_state)\n",
    "conv5_2 = conv_layer(conv5_1, \"conv5_2\",training_state)\n",
    "conv5_3 = conv_layer(conv5_2, \"conv5_3\",training_state)\n",
    "pool5, pool5_index,shape_5 = max_pool(conv5_3, 'pool5')\n",
    "        \n",
    "        \n",
    "    #---------------------So Now the encoder process has been Finished--------------------------------------#\n",
    "    #------------------Then Let's start Decoder Process-----------------------------------------------------#\n",
    "    \n",
    "    #First box of decovolution layers(3)\n",
    "deconv1_1 = up_sampling(pool5, pool5_index,shape=shape_5)\n",
    "deconv1_2 = deconv_layer(deconv1_1,[3,3,512,512],shape_5,\"deconv1_2\",training_state)\n",
    "deconv1_3 = deconv_layer(deconv1_2,[3,3,512,512],shape_5,\"deconv1_3\",training_state)\n",
    "deconv1_4 = deconv_layer(deconv1_3,[3,3,512,512],shape_5,\"deconv1_4\",training_state)\n",
    "    \n",
    "    #Second box of deconvolution layers(6)\n",
    "deconv2_1 = up_sampling(deconv1_4,pool4_index,shape = shape_4)\n",
    "deconv2_2 = deconv_layer(deconv2_1,[3,3,512,512],shape_4,\"deconv2_2\",training_state)\n",
    "deconv2_3 = deconv_layer(deconv2_2,[3,3,512,512],shape_4,\"deconv2_3\",training_state)\n",
    "deconv2_4 = deconv_layer(deconv2_3,[3,3,256,512],[1,45,60,256],\"deconv2_4\",training_state)\n",
    "   \n",
    "    #Third box of deconvolution layers(9)\n",
    "deconv3_1 = up_sampling(deconv2_4,pool3_index,shape = shape_3)\n",
    "deconv3_2 = deconv_layer(deconv3_1,[3,3,256,256],shape_3,\"deconv3_2\",training_state)\n",
    "deconv3_3 = deconv_layer(deconv3_2,[3,3,256,256],shape_3,\"deconv3_3\",training_state)\n",
    "\n",
    "deconv3_4 = deconv_layer(deconv3_3,[3,3,128,256],[1,90,120,128],\"deconv3_4\",training_state)\n",
    "    \n",
    "    #Fourth box of deconvolution layers(11)\n",
    "deconv4_1 = up_sampling(deconv3_4,pool2_index,shape = shape_2)\n",
    "deconv4_2 = deconv_layer(deconv4_1,[3,3,128,128],shape_2,\"deconv4_2\",training_state)\n",
    "deconv4_3 = deconv_layer(deconv4_2,[3,3,64,128],[1,180,240,64],\"deconv4_3\",training_state)\n",
    "    \n",
    "    #Fifth box of deconvolution layers(13)\n",
    "deconv5_1 = up_sampling(deconv4_3,pool1_index,shape = shape_1)\n",
    "deconv5_2 = deconv_layer(deconv5_1,[3,3,64,64],shape_1,\"deconv5_2\",training_state)\n",
    "deconv5_3 = deconv_layer(deconv5_2,[3,3,11,64],[shape_1[0],shape_1[1],shape_1[2],11],\"deconv5_3\",training_state)\n",
    "    \n",
    "prob = tf.nn.softmax(deconv5_3,name = \"prob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 23, 30, 512] [1, 45, 60, 512] [1, 90, 120, 256]\n"
     ]
    }
   ],
   "source": [
    "print(shape_5, shape_4, shape_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tnowak/.conda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Tensor(\"prob:0\", shape=(1, 360, 480, 11), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(prob)\n",
    "    result = sess.run(tf.argmax(prob, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Utils import writeImage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeImage(result[0], 'pred_image.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
